# âš¡ ì—˜ë¦¬ìŠ¤ í´ë¼ìš°ë“œ ë¹ ë¥¸ ì‹œì‘ (5ë¶„ ì„¤ì •)

## ğŸ¯ í•œ ì¤„ ì„¤ì¹˜ (ë³µì‚¬í•´ì„œ í„°ë¯¸ë„ì— ë¶™ì—¬ë„£ê¸°)

```bash
curl -sSL https://raw.githubusercontent.com/seoneum/rl_ars/standing-improvement/setup_elice.sh | bash
```

ë˜ëŠ” ìˆ˜ë™ìœ¼ë¡œ:

```bash
# 1. ì„¤ì¹˜ ìŠ¤í¬ë¦½íŠ¸ ë‹¤ìš´ë¡œë“œ
wget https://raw.githubusercontent.com/seoneum/rl_ars/standing-improvement/setup_elice.sh

# 2. ì‹¤í–‰ ê¶Œí•œ ë¶€ì—¬
chmod +x setup_elice.sh

# 3. ì‹¤í–‰
./setup_elice.sh
```

---

## ğŸš€ ì„¤ì¹˜ í›„ ë°”ë¡œ í•™ìŠµ ì‹œì‘

```bash
# í”„ë¡œì íŠ¸ ë””ë ‰í† ë¦¬ë¡œ ì´ë™
cd ~/rl_ars

# ê°„í¸ ì‹¤í–‰ (í™˜ê²½ ìë™ ì„¤ì •)
./start_training.sh
```

---

## ğŸ“Š GPUë³„ ìµœì  ëª…ë ¹ì–´

### A100 ì„œë²„
```bash
cd ~/rl_ars
source .venv/bin/activate
python train_a100_optimized.py phase1
```

### V100 ì„œë²„
```bash
cd ~/rl_ars
source .venv/bin/activate
python mjx_ars_train.py \
    --xml quadruped.xml \
    --num-envs 512 \
    --num-dirs 32 \
    --save-path v100_policy.npz
```

### T4 / ì¼ë°˜ GPU
```bash
cd ~/rl_ars
source .venv/bin/activate
./run_training.sh phase1
```

---

## ğŸ” ì„¤ì¹˜ í™•ì¸

```bash
cd ~/rl_ars
source .venv/bin/activate
python -c "
import jax
print('JAX devices:', jax.devices())
print('GPU available:', any(d.device_kind=='gpu' for d in jax.devices()))
"
```

---

## ğŸ’¡ VS Code í„°ë¯¸ë„ ë‹¨ì¶•í‚¤ ì„¤ì •

VS Codeì—ì„œ ì‘ì—…í•˜ê¸° í¸í•˜ê²Œ:

1. **í„°ë¯¸ë„ ì—´ê¸°**: `` Ctrl + ` ``
2. **ìƒˆ í„°ë¯¸ë„**: `Ctrl + Shift + ` ``
3. **í„°ë¯¸ë„ ë¶„í• **: `Ctrl + Shift + 5`

### ìë™ í™˜ê²½ í™œì„±í™” ì„¤ì •

VS Code ì„¤ì • (settings.json):
```json
{
    "terminal.integrated.shellArgs.linux": [
        "-c",
        "cd ~/rl_ars && source .venv/bin/activate && exec bash"
    ]
}
```

---

## ğŸ–¥ï¸ tmuxë¡œ ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰ (ì„¸ì…˜ ìœ ì§€)

```bash
# í•™ìŠµì„ tmux ì„¸ì…˜ì—ì„œ ì‹œì‘ (ì—°ê²° ëŠì–´ë„ ê³„ì† ì‹¤í–‰)
cd ~/rl_ars
./tmux_training.sh

# í•™ìŠµ ìƒíƒœ í™•ì¸
tmux attach -t training

# tmux ì„¸ì…˜ì—ì„œ ë‚˜ê°€ê¸° (í•™ìŠµì€ ê³„ì†)
Ctrl+B, ê·¸ ë‹¤ìŒ D
```

---

## ğŸ“ˆ ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§

í„°ë¯¸ë„ì„ 3ê°œë¡œ ë¶„í• í•˜ì—¬:

**í„°ë¯¸ë„ 1 - í•™ìŠµ**
```bash
cd ~/rl_ars
./run_training.sh phase1
```

**í„°ë¯¸ë„ 2 - GPU ëª¨ë‹ˆí„°ë§**
```bash
watch -n 1 nvidia-smi
```

**í„°ë¯¸ë„ 3 - ë¡œê·¸ í™•ì¸**
```bash
cd ~/rl_ars
tail -f *.log  # ë¡œê·¸ íŒŒì¼ì´ ìˆëŠ” ê²½ìš°
```

---

## âš ï¸ ìì£¼ ë°œìƒí•˜ëŠ” ë¬¸ì œì™€ í•´ê²°

### 1. "No GPU detected"
```bash
# CUDA ì¬ì„¤ì •
export CUDA_HOME=/usr/local/cuda-12.0
export PATH=$CUDA_HOME/bin:$PATH
export LD_LIBRARY_PATH=$CUDA_HOME/lib64:$LD_LIBRARY_PATH

# JAX ì¬ì„¤ì¹˜
source .venv/bin/activate
uv pip uninstall jax jaxlib -y
uv pip install --upgrade "jax[cuda12_pip]" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html
```

### 2. "Out of Memory (OOM)"
```bash
# ë°°ì¹˜ í¬ê¸° ì¤„ì´ê¸°
python mjx_ars_train.py \
    --xml quadruped.xml \
    --num-envs 64 \     # 128 â†’ 64
    --num-dirs 8 \      # 16 â†’ 8
    --save-path small_batch.npz
```

### 3. "Module not found"
```bash
# ê°€ìƒí™˜ê²½ ì¬í™œì„±í™”
cd ~/rl_ars
deactivate  # ê¸°ì¡´ í™˜ê²½ ì¢…ë£Œ
source .venv/bin/activate
source env.sh
```

---

## ğŸ“ ì²´í¬ë¦¬ìŠ¤íŠ¸

- [ ] GPU í™•ì¸: `nvidia-smi`
- [ ] CUDA í™•ì¸: `nvcc --version`
- [ ] Python 3.11: `python --version`
- [ ] JAX GPU: `python -c "import jax; print(jax.devices())"`
- [ ] í”„ë¡œì íŠ¸ ìœ„ì¹˜: `~/rl_ars`
- [ ] ê°€ìƒí™˜ê²½ í™œì„±í™”: `.venv/bin/activate`

---

## ğŸ‰ ì™„ë£Œ!

ëª¨ë“  ì„¤ì •ì´ ëë‚¬ìŠµë‹ˆë‹¤. ì´ì œ í•™ìŠµì„ ì‹œì‘í•˜ì„¸ìš”:

```bash
cd ~/rl_ars
./start_training.sh
```

**Success Criteria:**
- Phase 1: 35-45 reward points (300 iterations)
- Phase 2: 65-75+ reward points (400 iterations)

---

## ğŸ“ ë„ì›€ë§

ë¬¸ì œê°€ ë°œìƒí•˜ë©´:
1. ì„¤ì¹˜ ë¡œê·¸ í™•ì¸: `cat ~/rl_ars/installation.log`
2. GPU ìƒíƒœ í™•ì¸: `nvidia-smi`
3. GitHub Issues: https://github.com/seoneum/rl_ars/issues